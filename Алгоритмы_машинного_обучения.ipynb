{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Алгоритмы машинного обучения.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1AKgsPoEIK6X"
      },
      "source": [
        "Разные полезные ссылки\r\n",
        "\r\n",
        "https://github.com/3Top/word2vec-api#where-to-get-a-pretrained-models — пре-тренированные word2vec\r\n",
        "\r\n",
        "https://habr.com/ru/company/ods/blog/325422/ — фичи и трюки при работе с данными\r\n",
        "\r\n",
        "https://github.com/blue-yonder/tsfresh — библиотека для извлечение признаков из временных рядов\r\n",
        "\r\n",
        "https://www.kdnuggets.com/2021/01/ultimate-scikit-learn-machine-learning-cheatsheet.html — большой cheat-sheet по sklearnу"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NA-MYk-9idFn"
      },
      "source": [
        "# Алгоритмы с учителем"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RFGERalEiirl"
      },
      "source": [
        "## **Логистическая регрессия**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sb9PJE8CipBv"
      },
      "source": [
        "Логистическая регрессия представляет собой одну из моделей, которые относятся к линейному классификатору. Простыми словами, задачей линейного классификатора является предсказание целевых значений **y** от переменных **X** (регрессоров). При этом считается, что зависимость между признаками **X** и целевыми значениями y линейная. Отсюда собственно и название классификатора — линейный. Если очень грубо обобщить, то в основе модели логистической регрессии лежит предположение о наличии линейной зависимости между признаками **X** и целевыми значениями **y** . Вот она — связь.\r\n",
        "https://habr.com/ru/company/ods/blog/323890/#2-logisticheskaya-regressiya\r\n",
        "\r\n",
        "Плюсы:\r\n",
        "> *   Хорошо изучены\r\n",
        "*   Очень быстрые, могут работать на очень больших выборках\r\n",
        "* Практически вне конкуренции, когда признаков очень много (от сотен тысяч и более), и они разреженные (хотя есть еще факторизационные машины)\r\n",
        "* Коэффициенты перед признаками могут интерпретироваться (при условии что признаки масштабированы) – в линейной регрессии как частные производные зависимой переменной от признаков, в логистической – как изменение шансов на отнесение к одному из классов в раз при изменении признака на 1 ед., подробнее тут\r\n",
        "* Логистическая регрессия выдает вероятности отнесения к разным классам (это очень ценится, например, в кредитном скоринге)\r\n",
        "* Модель может строить и нелинейную границу, если на вход подать полиномиальные признаки\r\n",
        "\r\n",
        "Минусы:\r\n",
        ">* Плохо работают в задачах, в которых зависимость ответов от признаков сложная, нелинейная\r\n",
        "*  На практике предположения теоремы Маркова-Гаусса почти никогда не выполняются, поэтому чаще линейные методы работают хуже, чем, например, SVM и ансамбли (по качеству решения задачи классификации/регрессии)\r\n",
        "\r\n",
        "Как импортировать из sklearn\r\n",
        "\r\n",
        "\r\n",
        "```\r\n",
        "from sklearn.linear_model import Logistic Regression\r\n",
        "```\r\n",
        "\r\n",
        "Параметры не имеет смысла тюнить, кроме параметра C, который отвечает за регуляризацию\r\n",
        "\r\n",
        "Линейные методы не умеют разделять многомерные пространства, не могут решать XOR-проблему\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-XE1QaIbcAa7"
      },
      "source": [
        "## **Деревья решений**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d0EZ5r-LcNOC"
      },
      "source": [
        "Деревья решений используются в повседневной жизни в самых разных областях человеческой деятельности, порой и очень далеких от машинного обучения. Деревом решений можно назвать наглядную инструкцию, что делать в какой ситуации.\r\n",
        "https://habr.com/ru/company/ods/blog/322534/#derevo-resheniy\r\n",
        "\r\n",
        "Основные параметры класса sklearn.tree.DecisionTreeClassifier:\r\n",
        "\r\n",
        ">  max_depth – максимальная глубина дерева\r\n",
        ">\r\n",
        ">  max_features — максимальное число признаков, по которым ищется лучшее разбиение в дереве (это нужно потому, что при большом количестве признаков будет \"дорого\" искать лучшее (по критерию типа прироста информации) разбиение среди всех признаков)\r\n",
        ">  \r\n",
        ">  min_samples_leaf – минимальное число объектов в листе. У этого параметра есть понятная интерпретация: скажем, если он равен 5, то дерево будет порождать только те классифицирующие правила, которые верны как минимум для 5 объектов\r\n",
        "\r\n",
        "Плюсы:\r\n",
        "\r\n",
        "*   Порождение четких правил классификации, понятных человеку, например, \"если возраст < 25 и интерес к мотоциклам, то отказать в кредите\". Это свойство называют интерпретируемостью модели;\r\n",
        "\r\n",
        "*   Деревья решений могут легко визуализироваться, то есть может \"интерпретироваться\" (строгого определения я не видел) как сама модель (дерево), так и прогноз для отдельного взятого тестового объекта (путь в дереве);\r\n",
        "\r\n",
        "* Быстрые процессы обучения и прогнозирования;\r\n",
        "\r\n",
        "* Малое число параметров модели;\r\n",
        "\r\n",
        "* Поддержка и числовых, и категориальных признаков.\r\n",
        "\r\n",
        "Минусы: \r\n",
        "\r\n",
        "* У порождения четких правил классификации есть и другая сторона: деревья очень чувствительны к шумам во входных данных, вся модель может кардинально измениться, если немного изменится обучающая выборка (например, если убрать один из признаков или добавить несколько объектов), поэтому и правила классификации могут сильно изменяться, что ухудшает интерпретируемость модели;\r\n",
        "\r\n",
        "* Разделяющая граница, построенная деревом решений, имеет свои ограничения (состоит из гиперплоскостей, перпендикулярных какой-то из координатной оси), и на практике дерево решений по качеству классификации уступает некоторым другим методам;\r\n",
        "\r\n",
        "* Необходимость отсекать ветви дерева (pruning) или устанавливать минимальное число элементов в листьях дерева или максимальную глубину дерева для борьбы с переобучением. Впрочем, переобучение — проблема всех методов машинного обучения;\r\n",
        "\r\n",
        "* Нестабильность. Небольшие изменения в данных могут существенно изменять построенное дерево решений. С этой проблемой борются с помощью ансамблей деревьев решений;\r\n",
        "\r\n",
        "* Проблема поиска оптимального дерева решений (минимального по размеру и способного без ошибок классифицировать выборку) NP-полна, поэтому на практике используются эвристики типа жадного поиска признака с максимальным приростом информации, которые не гарантируют нахождения глобально оптимального дерева;\r\n",
        "\r\n",
        "* Сложно поддерживаются пропуски в данных. Friedman оценил, что на поддержку пропусков в данных ушло около 50% кода CART (классический алгоритм построения деревьев классификации и регрессии – Classification And Regression Trees, в sklearn реализована улучшенная версия именно этого алгоритма);\r\n",
        "\r\n",
        "* Модель умеет только интерполировать, но не экстраполировать (это же верно и для леса и бустинга на деревьях). То есть дерево решений делает константный прогноз для объектов, находящихся в признаковом пространстве вне параллелепипеда, охватывающего все объекты обучающей выборки."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sSotXH3xmnMQ"
      },
      "source": [
        "## **Метод k-ближайших соседей**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rOdIpi47mta3"
      },
      "source": [
        "Метод ближайших соседей (k Nearest Neighbors, или kNN) — тоже очень популярный метод классификации, также иногда используемый в задачах регрессии. Это, наравне с деревом решений, один из самых понятных подходов к классификации. На уровне интуиции суть метода такова: посмотри на соседей, какие преобладают, таков и ты. Формально основой метода является гипотеза компактности: если метрика расстояния между примерами введена достаточно удачно, то схожие примеры гораздо чаще лежат в одном классе, чем в разных. \r\n",
        "\r\n",
        "В чистом виде kNN может послужить хорошим стартом (baseline) в решении какой-либо задачи;\r\n",
        "\r\n",
        "\r\n",
        "Основные параметры класса sklearn.neighbors.KNeighborsClassifier:\r\n",
        "*   weights: \"uniform\" (все веса равны), \"distance\" (вес обратно пропорционален расстоянию до тестового примера) или другая определенная пользователем функция\r\n",
        "\r\n",
        "*   algorithm (опционально): \"brute\", \"ball_tree\", \"KD_tree\", или \"auto\". В первом случае ближайшие соседи для каждого тестового примера считаются перебором обучающей выборки. Во втором и третьем — расстояние между примерами хранятся в дереве, что ускоряет нахождение ближайших соседей. В случае указания параметра \"auto\" подходящий способ нахождения соседей будет выбран автоматически на основе обучающей выборки.\r\n",
        "\r\n",
        "*    leaf_size (опционально): порог переключения на полный перебор в случае выбора BallTree или KDTree для нахождения соседей\r\n",
        "\r\n",
        "*    metric: \"minkowski\", \"manhattan\", \"euclidean\", \"chebyshev\" и другие\r\n",
        "\r\n",
        "Плюсы:\r\n",
        "\r\n",
        "*    Простая реализация\r\n",
        "\r\n",
        "*    Неплохо изучен теоретически\r\n",
        "\r\n",
        "*    Как правило, метод хорош для первого решения задачи, причем не только классификации или регрессии, но и, например, рекомендации;\r\n",
        "\r\n",
        "*    Можно адаптировать под нужную задачу выбором метрики или ядра (в двух словах: ядро может задавать операцию сходства для сложных объектов типа графов, а сам подход kNN остается тем же). Кстати, профессор ВМК МГУ и опытный участник соревнований по анализу данных Александр Дьяконов любит самый простой kNN, но с настроенной метрикой сходства объектов. Можно почитать про некоторые его решения в этой статье;\r\n",
        "\r\n",
        "*    Неплохая интерпретация, можно объяснить, почему тестовый пример был классифицирован именно так. Хотя этот аргумент можно атаковать: если число соседей большое, то интерпретация ухудшается (условно: \"мы не дали ему кредит, потому что он похож на 350 клиентов, из которых 70 – плохие, что на 12% больше, чем в среднем по выборке\").\r\n",
        "\r\n",
        "Минусы:\r\n",
        "*    Метод считается быстрым в сравнении, например, с композициями алгоритмов, но в реальных задачах, как правило, число соседей, используемых для классификации, будет большим (100-150), и в таком случае алгоритм будет работать не так быстро, как дерево решений;\r\n",
        "\r\n",
        "*    Если в наборе данных много признаков, то трудно подобрать подходящие веса и определить, какие признаки не важны для классификации/регрессии;\r\n",
        "\r\n",
        "*    Зависимость от выбранной метрики расстояния между примерами. Выбор по умолчанию евклидового расстояния чаще всего ничем не обоснован. Можно отыскать хорошее решение перебором параметров, но для большого набора данных это отнимает много времени;\r\n",
        "\r\n",
        "*    Нет теоретических оснований выбора определенного числа соседей — только перебор (впрочем, чаще всего это верно для всех гиперпараметров всех моделей). В случае малого числа соседей метод чувствителен к выбросам, то есть склонен переобучаться;\r\n",
        "\r\n",
        "*    Как правило, плохо работает, когда признаков много, из-за \"проклятия размерности\". Про это хорошо рассказывает известный в ML-сообществе профессор Pedro Domingos – тут в популярной статье \"A Few Useful Things to Know about Machine Learning\", также \"the curse of dimensionality\" описывается в книге Deep Learning в главе \"Machine Learning basics\".\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NdeA4Bl08U3a"
      },
      "source": [
        "## **Ансамбли**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dBPPVIH68_f1"
      },
      "source": [
        "### **Бэггинг** "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bOJntdrD9IEd"
      },
      "source": [
        "В библиотеке scikit-learn есть реализации BaggingRegressor и BaggingClassifier, которые позволяют использовать большинство других алгоритмов \"внутри\".\r\n",
        "По факту беггинг это композиция нескольких алгоритмов, которые беруют случайные подвыборки из выборки (с учётом повторяющихся данных) и поэтому беггинг не эффективен на больших выборках\r\n",
        "Информация с: https://habr.com/ru/company/ods/blog/324402/ "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_P2EeDz1A-1u"
      },
      "source": [
        "### **Случайный лес**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4UQBD4Y5Bkuc"
      },
      "source": [
        "Решающие деревья являются хорошим семейством базовых классификаторов для бэггинга, поскольку они достаточно сложны и могут достигать нулевой ошибки на любой выборке. Метод случайных подпространств позволяет снизить коррелированность между деревьями и избежать переобучения. Базовые алгоритмы обучаются на различных подмножествах признакового описания, которые также выделяются случайным образом.\r\n",
        "\r\n",
        "Параметры на которые стоит обращать внимание при построении модели первым делом:\r\n",
        "\r\n",
        "*    n_estimators — число деревьев в \"лесу\" (100 опт)\r\n",
        "\r\n",
        "*    criterion — критерий для разбиения выборки в вершине\r\n",
        "\r\n",
        "*    max_features — число признаков, по которым ищется разбиение (5-10 опт)\r\n",
        "\r\n",
        "*    min_samples_leaf — минимальное число объектов в листе (1-3 опт)\r\n",
        "*    max_depth — максимальная глубина дерева (15-20 опт)\r\n",
        "\r\n",
        "В случае сильного переобучения при использовании случайного леса или градиентного бустинга стоит использовать сверхслучайный лес или ExtraTreesClassifier/Regressor\r\n",
        "\r\n",
        "**Плюсы:**\r\n",
        "*   имеет высокую точность предсказания, на большинстве задач будет лучше линейных алгоритмов; точность сравнима с точностью бустинга\r\n",
        "\r\n",
        "*   практически не чувствителен к выбросам в данных из-за случайного сэмлирования\r\n",
        "\r\n",
        "*   не чувствителен к масштабированию (и вообще к любым монотонным преобразованиям) значений признаков, связано с выбором случайных подпространств\r\n",
        "*   не требует тщательной настройки параметров, хорошо работает «из коробки». С помощью «тюнинга» параметров можно достичь прироста от 0.5 до 3% точности в зависимости от задачи и данных\r\n",
        "\r\n",
        "*   способен эффективно обрабатывать данные с большим числом признаков и классов\r\n",
        "\r\n",
        "*   одинаково хорошо обрабатывет как непрерывные, так и дискретные признаки\r\n",
        "\r\n",
        "*   редко переобучается, на практике добавление деревьев почти всегда только улучшает композицию, но на валидации, после достижения определенного количества деревьев, кривая обучения выходит на асимптоту\r\n",
        "\r\n",
        "*   для случайного леса существуют методы оценивания значимости отдельных признаков в модели\r\n",
        "\r\n",
        "*   хорошо работает с пропущенными данными; сохраняет хорошую точность, если большая часть данных пропущенна\r\n",
        "\r\n",
        "*   предполагает возможность сбалансировать вес каждого класса на всей выборке, либо на подвыборке каждого дерева\r\n",
        "\r\n",
        "*   вычисляет близость между парами объектов, которые могут использоваться при кластеризации, обнаружении выбросов или (путем масштабирования) дают интересные представления данных\r\n",
        "\r\n",
        "*   возможности, описанные выше, могут быть расширены до неразмеченных данных, что приводит к возможности делать кластеризацию и визуализацию данных, обнаруживать выбросы\r\n",
        "\r\n",
        "*   высокая параллелизуемость и масштабируемость.\r\n",
        "\r\n",
        "**Минусы:**\r\n",
        "*   в отличие от одного дерева, результаты случайного леса сложнее интерпретировать\r\n",
        "\r\n",
        "*   нет формальных выводов (p-values), доступных для оценки важности переменных\r\n",
        "\r\n",
        "*   алгоритм работает хуже многих линейных методов, когда в выборке очень много разреженных признаков (тексты, Bag of words)\r\n",
        "\r\n",
        "*   случайный лес не умеет экстраполировать, в отличие от той же линейной регрессии (но это можно считать и плюсом, так как не будет экстремальных значений в случае попадания выброса)\r\n",
        "\r\n",
        "*   алгоритм склонен к переобучению на некоторых задачах, особенно на зашумленных данных\r\n",
        "\r\n",
        "*   для данных, включающих категориальные переменные с различным количеством уровней, случайные леса предвзяты в пользу признаков с большим количеством уровней: когда у признака много уровней, дерево будет сильнее подстраиваться именно под эти признаки, так как на них можно получить более высокое значение оптимизируемого функционала (типа прироста информации)\r\n",
        "\r\n",
        "*   если данные содержат группы коррелированных признаков, имеющих схожую значимость для меток, то предпочтение отдается небольшим группам перед большими\r\n",
        "\r\n",
        "*   больший размер получающихся моделей.\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uUrP8URLcefu"
      },
      "source": [
        "## **Градиентный бустинг**\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uCsFNNOzL0ED"
      },
      "source": [
        "# Обучение без учителя\r\n",
        "Основное отличие методов обучения без учителя от привычных классификаций и регрессий машинного обучения в том, что разметки для данных в этом случае нет. От этого образуются сразу несколько особенностей — во-первых это возможность использования несопоставимо больших объёмов данных, поскольку их не нужно будет размечать руками для обучения, а во-вторых это неясность измерения качества методов, из-за отсутствия таких же прямолинейных и интуитивно понятных метрик, как в задачах обучения с учителем.\r\n",
        "\r\n",
        "источник — https://habr.com/ru/company/ods/blog/325654/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BQ0jyBerL1vW"
      },
      "source": [
        "## **Метод главных компонент (PCA)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C7jxfkzfL8dh"
      },
      "source": [
        "Метод главных компонент (Principal Component Analysis) — один из самых интуитивно простых и часто используемых методов для снижения размерности данных и проекции их на ортогональное подпространство признаков.\r\n",
        "\r\n",
        "В совсем общем виде это можно представить как предположение о том, что все наши наблюдения скорее всего выглядят как некий эллипсоид в подпространстве нашего исходного пространства и наш новый базис в этом пространстве совпадает с осями этого эллипсоида. Это предположение позволяет нам одновременно избавиться от сильно скоррелированных признаков, так как вектора базиса пространства, на которое мы проецируем, будут ортогональными\r\n",
        "\r\n",
        "Для сложных данных высокой размерности, где данные не разбиваются тривиально вдоль одного признака, применение PCA может достаточно сильно улучшить качество работы деревьев решений и ансамблей на их основе.\r\n",
        "\r\n",
        "PCA находится в sklearn.decomposition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4VdGuvmhQEnc"
      },
      "source": [
        "## **Кластеризация**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fP5EEoBAQILv"
      },
      "source": [
        "Интуитивная постановка задачи кластеризации довольно проста и представляет из себя наше желание сказать: \"Вот тут у меня насыпаны точки. Я вижу, что они сваливаются в какие-то кучки вместе. Было бы круто иметь возможность эти точки относить к кучкам и в случае появления новой точки на плоскости говорить, в какую кучку она падает.\" Из такой постановки видно, что пространства для фантазии получается много, и от этого возникает соответствующее множество алгоритмов решения этой задачи. Перечисленные алгоритмы ни в коем случае не описывают данное множество полностью, но являются примерами самых популярных методов решения задачи кластеризации.\r\n",
        "\r\n",
        "**K-means**\r\n",
        "\r\n",
        "Выбор числа кластеров для kMeans\r\n",
        "\r\n",
        "В отличие от задачи классификации или регресии, в случае кластеризации сложнее выбрать критерий, с помощью которого было бы просто представить задачу кластеризации как задачу оптимизации.\r\n",
        "В случае kMeans распространен вот такой критерий – сумма квадратов расстояний от точек до центроидов кластеров, к которым они относятся.\r\n",
        "\r\n",
        "Пример:\r\n",
        "\r\n",
        "\r\n",
        "```\r\n",
        "from sklearn.cluster import KMeans\r\n",
        "\r\n",
        "inertia = []\r\n",
        "for k in range(1, 8):\r\n",
        "    kmeans = KMeans(n_clusters=k, random_state=1).fit(X)\r\n",
        "    inertia.append(np.sqrt(kmeans.inertia_))\r\n",
        "\r\n",
        "plt.plot(range(1, 8), inertia, marker='s');\r\n",
        "plt.xlabel('$k$')\r\n",
        "plt.ylabel('$J(C_k)$');\r\n",
        "```\r\n",
        "Видим, что J(Ck) падает сильно при увеличении числа кластеров с 1 до 2 и с 2 до 3 и уже не так сильно – при изменении с 3 до 4. Значит, в данной задаче оптимально задать 3 кластера.\r\n",
        "\r\n",
        "\r\n",
        "Плюсы:\r\n",
        "*   Прост в реализации\r\n",
        "\r\n",
        "*   Прост в объяснении\r\n",
        "\r\n",
        "*   Хорошо изучен\r\n",
        "\r\n",
        "Минусы:\r\n",
        "*  Чувствителен к начальному положению центроида в кластере\r\n",
        "\r\n",
        "*  Вычислительно трудный\r\n",
        "\r\n",
        "**Affinity Propagation**\r\n",
        "\r\n",
        "Ещё один пример алгоритма кластеризации. В отличие от алгоритма К-средних, данный подход не требует заранее определять число кластеров, на которое мы хотим разбить наши данные. Основная идея алгоритма заключается в том, что нам хотелось бы, чтобы наши наблюдения кластеризовались в группы на основе того, как они \"общаются\", или насколько они похожи друг на друга.\r\n",
        "\r\n",
        "**Спектральная кластеризация**\r\n",
        "\r\n",
        "Спектральная кластеризация объединяет несколько описанных выше подходов, чтобы получить максимальное количество профита от сложных многообразий размерности меньшей исходного пространства.\r\n",
        "\r\n",
        "Для работы этого алгоритма нам потребуется определить матрицу похожести наблюдений (adjacency matrix)\r\n",
        "\r\n",
        "**Агломеративная кластеризация**\r\n",
        "\r\n",
        "Наверное самый простой и понятный алгоритм кластеризации без фиксированного числа кластеров — агломеративная кластеризация. Интуиция у алгоритма очень простая:\r\n",
        "*    Начинаем с того, что высыпаем на каждую точку свой кластер\r\n",
        "*    Сортируем попарные расстояния между центрами кластеров по возрастанию\r\n",
        "*    Берём пару ближайших кластеров, склеиваем их в один и пересчитываем центр кластера\r\n",
        "*    Повторяем п. 2 и 3 до тех пор, пока все данные не склеятся в один кластер\r\n",
        "\r\n",
        "Профит первых трёх подходов по сравнению с четвёртым в том, что для них не нужно будет пересчитывать расстояния каждый раз после склеивания, что сильно снижает вычислительную сложность алгоритма.\r\n",
        "\r\n",
        "По итогам выполнения такого алгоритма можно также построить замечательное дерево склеивания кластеров и глядя на него определить, на каком этапе нам было бы оптимальнее всего остановить алгоритм. Либо воспользоваться тем же правилом локтя, что и в k-means.\r\n",
        "\r\n",
        "При отсутствии истинного кол-ва кластеров лучшая метрика для кластеризации — Силуэт"
      ]
    }
  ]
}